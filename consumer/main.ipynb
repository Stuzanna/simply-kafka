{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "A broken up version of [main.py](./main.py) that shows how put it together and alternatives explored.\n",
    "\n",
    "1. Created in Kafka Python from an Aiven exmaple, found this was too old so rebuilt it using the;\n",
    "1. Confluent Python, kafka-python is incompatible or poorly maintained\n",
    "1. Also see the Quix consumer example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confluent Python\n",
    "The OSS Kafka Python library is unfortunately stale (public release 2020) and not great, so it's best to use Confluent's based on librdkafka.\n",
    "\n",
    "[Docs](https://docs.confluent.io/kafka-clients/python/current/overview.html), and [Github](https://github.com/confluentinc/confluent-kafka-python).\n",
    "\n",
    "## A note on resetting offsets\n",
    "\n",
    "I was getting confused with `auto.offset.reset=earliest` not causing a reset. This is because this property does not cause a reset! It is a common source of confusion. The property is for when a new, i.e. an unknown consumer group appears to Kafka, such as the first time you start comitting offsets. It is to define behaviour when you start comitting.\n",
    "\n",
    "If instead you would like to re-read everything , as I did in development here, you should stop the app and reset offsets (using a tool such as [Conduktor](https://conduktor.io))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install confluent-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_servers = \"localhost:19092,\"\n",
    "topics = [\"weather_data_demo\"]\n",
    "client_id = \"myClientId\"\n",
    "consumer_group = \"stuCG\"\n",
    "offset_config = \"earliest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# -- shell script to reset offsets --\n",
    "# if not using a tool\n",
    "kafka-consumer-groups.sh --bootstrap-server localhost:19092 --group stuCG --topic weather_data_demo --reset-offsets --to-earliest --execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Creating A Consumer ---\n",
    "\n",
    "from confluent_kafka import DeserializingConsumer\n",
    "import json\n",
    "import sys\n",
    "\n",
    "def json_serializer(msg, s_obj=None):\n",
    "    # return json.loads(msg.decode('ascii')) # alternative data type\n",
    "    return json.loads(msg)\n",
    "\n",
    "conf = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'client.id': client_id,\n",
    "    'group.id': consumer_group,\n",
    "    # 'security.protocol': 'SSL',\n",
    "    # 'ssl.ca.location': '../sslcerts/ca.pem',\n",
    "    # 'ssl.certificate.location': '../sslcerts/service.cert',\n",
    "    # 'ssl.key.location': '../sslcerts/service.key', \n",
    "    # 'key.deserializer': json_serializer, # if key in JSON\n",
    "    'value.deserializer': json_serializer,\n",
    "    'auto.offset.reset': offset_config,\n",
    "    }\n",
    "\n",
    "consumer = DeserializingConsumer(conf)\n",
    "\n",
    "\n",
    "# --- Running the consumer ---\n",
    "\n",
    "running = True\n",
    "\n",
    "try:\n",
    "    consumer.subscribe(topics)\n",
    "    print(f\"Subscribed to topics: {topics}\")\n",
    "\n",
    "    while running:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg is None: \n",
    "            print(\"Waiting for message...\")\n",
    "            continue\n",
    "\n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                # End of partition event\n",
    "                sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                 (msg.topic(), msg.partition(), msg.offset()))\n",
    "            elif msg.error():\n",
    "                raise KafkaException(msg.error())\n",
    "        else:\n",
    "             print(f\"{msg.partition()}:{msg.offset()}: \"\n",
    "                  f\"k={msg.key()} \"\n",
    "                  f\"v={msg.value()}\")\n",
    "finally:\n",
    "    # Close down consumer to commit final offsets.\n",
    "    consumer.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Running the consumer ---\n",
    "\n",
    "running = True\n",
    "\n",
    "try:\n",
    "    consumer.subscribe([\"weather_data_demo\"])\n",
    "\n",
    "    while running:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg is None: continue\n",
    "\n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                # End of partition event\n",
    "                sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                 (msg.topic(), msg.partition(), msg.offset()))\n",
    "            elif msg.error():\n",
    "                raise KafkaException(msg.error())\n",
    "        else:\n",
    "            print (\"%d:%d: k=%s v=%s\" % (\n",
    "                msg.partition(),\n",
    "                msg.offset(),\n",
    "                msg.key(),\n",
    "                msg.value()))\n",
    "finally:\n",
    "    # Close down consumer to commit final offsets.\n",
    "    consumer.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Python\n",
    "From [kafka-python](https://github.com/dpkp/kafka-python).\n",
    "\n",
    "Tried to get this working, think it is failing on the metadata API so it can't discover the broker on this newer version of Kafka.\n",
    "Will get a `NoBrokersAvailable` error so moved to the Confluent python libray featured above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaAdminClient\n",
    "from kafka.admin import NewTopic\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostname = \"localhost\"\n",
    "port = \"19092\"\n",
    "topics = [\"weather_data_demo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the consumer\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "'''\n",
    "I think there's an issue with the metadata API call and the `kafka` library.\n",
    "No brokers available I think it's failing on that.\n",
    "'''\n",
    "\n",
    "def json_serialiser(msg, s_obj):\n",
    "    return json.loads(msg.decode('ascii'))\n",
    "\n",
    "config = {\n",
    "    'bootstrap_servers': hostname+\":\"+port,\n",
    "    'client_id': 'myClient',\n",
    "    'group_id': 'ConsumerAlpha',\n",
    "    # 'security.protocol': 'SSL',\n",
    "    # 'ssl.ca.location': '../sslcerts/ca.pem',\n",
    "    # 'ssl.certificate.location': '../sslcerts/service.cert',\n",
    "    # 'ssl.key.location': '../sslcerts/service.key', \n",
    "    'value_deserializer': json_serialiser,\n",
    "    'key_deserializer': json_serialiser\n",
    "}\n",
    "\n",
    "# consumer = KafkaConsumer(config)\n",
    "c2 = KafkaConsumer(\"weather_data_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the consumer\n",
    "\n",
    "try:\n",
    "    consumer.subscribe((topics))\n",
    "\n",
    "    while running:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg is None: \n",
    "            print(\"Waiting for message\")\n",
    "            continue\n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                # End of partition event\n",
    "                sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                 (msg.topic(), msg.partition(), msg.offset()))\n",
    "            elif msg.error():\n",
    "                raise KafkaException(msg.error())\n",
    "        else:\n",
    "            # print (\"%d:%d: k=%s v=%s\" % (\n",
    "            #     msg.partition(),\n",
    "            #     msg.offset(),\n",
    "            #     msg.key(),\n",
    "            #     msg.value()))\n",
    "            # test the udpate to f-strings\n",
    "            print(f\"{msg.partition()}:{msg.offset()}: \"\n",
    "                  f\"k={msg.key()} \"\n",
    "                  f\"v={msg.value()}\")\n",
    "\n",
    "finally:\n",
    "    # Close down consumer to commit final offsets.\n",
    "    consumer.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quix Consumer\n",
    "Code is an adapted version from [simple-kafka-python](https://github.com/quixio/simple-kafka-python/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quixstreams import Application\n",
    "import json\n",
    "import time\n",
    "# Application top level object\n",
    "\n",
    "app = Application (\n",
    "    broker_address = \"localhost:19092\",\n",
    "    loglevel = \"DEBUG\",\n",
    "    consumer_group = \"stu_consumer_group\",\n",
    "    auto_offset_reset = \"earliest\"\n",
    ")\n",
    "\n",
    "\n",
    "max_records = 1000  # Maximum number of records to process\n",
    "start_time = time.time()  # Start time for elapsed time measurement\n",
    "max_duration = 100 # max running time\n",
    "\n",
    "records_processed = 0\n",
    "\n",
    "with app.get_consumer() as consumer:\n",
    "    consumer.subscribe([\"customers\"]) # subscribing to a topic\n",
    "\n",
    "    while True:\n",
    "        msg = consumer.poll(1)\n",
    "        # breakpoint() # add for debugging\n",
    "        if msg is None:\n",
    "            print(\"Waiting for message...\")\n",
    "        elif msg.error() is not None:\n",
    "            raise Exception(msg.error())\n",
    "        # else:\n",
    "        #     breakpoint() # add for debugging\n",
    "        else:\n",
    "            key = msg.key().decode('utf8')\n",
    "            value = json.loads(msg.value())\n",
    "            offset = msg.offset()\n",
    "\n",
    "            print(f\"{offset} {'|'} {key} {'|'} {value} {'|'}\")\n",
    "            records_processed += 1\n",
    "\n",
    "        if records_processed >= max_records or time.time() - start_time >= max_duration:\n",
    "                print(\"Maximum records or time limit reached. Exiting...\")\n",
    "                break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiven-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
