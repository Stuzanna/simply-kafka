{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "Breaking up the code into reusable blocks.\n",
    "\n",
    "Confluent Python attempt is the version to use, kafka-python is incompatible.\n",
    "Also see the Quix consumer work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confluent Python\n",
    "The OSS Kafka Python library is unfortunately stale (public release 2020) and not great, so it's best to use Confluent's based on librdkafka.\n",
    "\n",
    "[Docs](https://docs.confluent.io/kafka-clients/python/current/overview.html), and [Github](https://github.com/confluentinc/confluent-kafka-python).\n",
    "\n",
    "I think that the offset_config doesn't always work, so added the shell command to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install confluent-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_servers = \"localhost:19092,\"\n",
    "topics = [\"weather_data_demo\"]\n",
    "client_id = \"myClientId\"\n",
    "consumer_group = \"stuCG\"\n",
    "offset_config = \"earliest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# shell script to reset offsets\n",
    "kafka-consumer-groups.sh --bootstrap-server localhost:19092 --group stuCG --topic weather_data_demo --reset-offsets --to-earliest --execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Blend of Aiven and Quix's demos ---\n",
    "\n",
    "# -- Creating A Consumer ---\n",
    "\n",
    "from confluent_kafka import DeserializingConsumer\n",
    "import json\n",
    "import sys\n",
    "\n",
    "def json_serializer(msg, s_obj=None):\n",
    "    # return json.loads(msg.decode('ascii'))\n",
    "    return json.loads(msg)\n",
    "\n",
    "conf = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'client.id': client_id,\n",
    "    'group.id': consumer_group,\n",
    "    # 'security.protocol': 'SSL',\n",
    "    # 'ssl.ca.location': '../sslcerts/ca.pem',\n",
    "    # 'ssl.certificate.location': '../sslcerts/service.cert',\n",
    "    # 'ssl.key.location': '../sslcerts/service.key', \n",
    "    'value.deserializer': json_serializer,\n",
    "    # 'key.deserializer': json_serializer, # if key in JSON\n",
    "    'auto.offset.reset': offset_config,\n",
    "    }\n",
    "\n",
    "consumer = DeserializingConsumer(conf)\n",
    "\n",
    "\n",
    "# --- Running the consumer ---\n",
    "\n",
    "running = True\n",
    "\n",
    "try:\n",
    "    consumer.subscribe(topics)\n",
    "    print(f\"Subscribed to topics: {topics}\")\n",
    "\n",
    "    while running:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg is None: \n",
    "            print(\"Waiting for message...\")\n",
    "            continue\n",
    "\n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                # End of partition event\n",
    "                sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                 (msg.topic(), msg.partition(), msg.offset()))\n",
    "            elif msg.error():\n",
    "                raise KafkaException(msg.error())\n",
    "        else:\n",
    "             print(f\"{msg.partition()}:{msg.offset()}: \"\n",
    "                  f\"k={msg.key()} \"\n",
    "                  f\"v={msg.value()}\")\n",
    "finally:\n",
    "    # Close down consumer to commit final offsets.\n",
    "    consumer.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Running the consumer ---\n",
    "\n",
    "running = True\n",
    "\n",
    "try:\n",
    "    consumer.subscribe([\"weather_data_demo\"])\n",
    "\n",
    "    while running:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg is None: continue\n",
    "\n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                # End of partition event\n",
    "                sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                 (msg.topic(), msg.partition(), msg.offset()))\n",
    "            elif msg.error():\n",
    "                raise KafkaException(msg.error())\n",
    "        else:\n",
    "            print (\"%d:%d: k=%s v=%s\" % (\n",
    "                msg.partition(),\n",
    "                msg.offset(),\n",
    "                msg.key(),\n",
    "                msg.value()))\n",
    "finally:\n",
    "    # Close down consumer to commit final offsets.\n",
    "    consumer.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Python\n",
    "From [kafka-python](https://github.com/dpkp/kafka-python).\n",
    "\n",
    "Tried to get this working, think it is failing on the metadata API so it can't discover the broker.\n",
    "Will get a `NoBrokersAvailable` error so moved to Confluent python libray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaAdminClient\n",
    "from kafka.admin import NewTopic\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostname = \"localhost\"\n",
    "port = \"19092\"\n",
    "topics = [\"weather_data_demo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the consumer\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "'''\n",
    "I think there's an issue with the metadata API call and the `kafka` library.\n",
    "No brokers available I think it's failing on that.\n",
    "'''\n",
    "\n",
    "def json_serialiser(msg, s_obj):\n",
    "    return json.loads(msg.decode('ascii'))\n",
    "\n",
    "config = {\n",
    "    'bootstrap_servers': hostname+\":\"+port,\n",
    "    'client_id': 'myClient',\n",
    "    'group_id': 'ConsumerAlpha',\n",
    "    # 'security.protocol': 'SSL',\n",
    "    # 'ssl.ca.location': '../sslcerts/ca.pem',\n",
    "    # 'ssl.certificate.location': '../sslcerts/service.cert',\n",
    "    # 'ssl.key.location': '../sslcerts/service.key', \n",
    "    'value_deserializer': json_serialiser,\n",
    "    'key_deserializer': json_serialiser\n",
    "}\n",
    "\n",
    "# consumer = KafkaConsumer(config)\n",
    "c2 = KafkaConsumer(\"weather_data_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the consumer\n",
    "\n",
    "try:\n",
    "    consumer.subscribe((topics))\n",
    "\n",
    "    while running:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg is None: \n",
    "            print(\"Waiting for message\")\n",
    "            continue\n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                # End of partition event\n",
    "                sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                 (msg.topic(), msg.partition(), msg.offset()))\n",
    "            elif msg.error():\n",
    "                raise KafkaException(msg.error())\n",
    "        else:\n",
    "            # print (\"%d:%d: k=%s v=%s\" % (\n",
    "            #     msg.partition(),\n",
    "            #     msg.offset(),\n",
    "            #     msg.key(),\n",
    "            #     msg.value()))\n",
    "            # test the udpate to f-strings\n",
    "            print(f\"{msg.partition()}:{msg.offset()}: \"\n",
    "                  f\"k={msg.key()} \"\n",
    "                  f\"v={msg.value()}\")\n",
    "\n",
    "finally:\n",
    "    # Close down consumer to commit final offsets.\n",
    "    consumer.close()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiven-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
